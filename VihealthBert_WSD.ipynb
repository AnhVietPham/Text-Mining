{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNWYvH837UKpZyIhK4lS9eM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnhVietPham/Text-Mining/blob/main/VihealthBert_WSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWZyAY_2QSed",
        "outputId": "09fcd000-9f63-45f6-facb-3a09e2173c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKNVIYvISdNH",
        "outputId": "cce540b9-f1fa-4cdc-c8cf-30a7bfdde2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel"
      ],
      "metadata": {
        "id": "wutn3WTGSWj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import re\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader\n",
        "from collections import defaultdict\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm, trange\n"
      ],
      "metadata": {
        "id": "q7wzLK7NStDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, config, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout_rate*2)\n",
        "        self.dense_1  = nn.Linear(config.hidden_size*2, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_2 = nn.Dropout(dropout_rate)\n",
        "        self.dense_2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, feature):\n",
        "\n",
        "        feature = self.dropout_1(feature)\n",
        "        feature = self.dense_1(feature)\n",
        "        feature = self.relu(feature)\n",
        "        feature = self.dropout_2(feature)\n",
        "        feature = self.dense_2(feature).view(-1)\n",
        "\n",
        "        feature = self.sigmoid(feature)\n",
        "        return feature"
      ],
      "metadata": {
        "id": "R2K49PT5SIu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViHnBERT(RobertaPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(ViHnBERT, self).__init__(config)\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        # init backbone\n",
        "        self.roberta = RobertaModel(config)\n",
        "\n",
        "        self.classifier = Classifier(config, args.dropout_rate)\n",
        "\n",
        "    \n",
        "    def forward(self,\n",
        "                input_ids=None, \n",
        "                token_type_ids=None, \n",
        "                attention_mask=None, \n",
        "                start_token_idx=None, \n",
        "                end_token_idx=None,\n",
        "                labels=None):\n",
        "\n",
        "        outputs = self.roberta(input_ids=input_ids,\n",
        "                                    attention_mask=attention_mask)\n",
        "\n",
        "        features_bert = outputs[0]\n",
        "        # Features of [CLS] tokens\n",
        "        features_cls = features_bert[:, 0, :].unsqueeze(1)\n",
        "\n",
        "        # Features of acronym tokens\n",
        "        if start_token_idx is None or end_token_idx is None:\n",
        "            raise Exception('Require start_token_idx and end_token_idx')\n",
        "        list_mean_feature_acr = []\n",
        "        for idx in range(features_bert.size()[0]):\n",
        "            feature_acr = features_bert[idx, start_token_idx[idx]:end_token_idx[idx]+1, :].unsqueeze(0)\n",
        "            mean_feature_acr = torch.mean(feature_acr, 1, True)\n",
        "            list_mean_feature_acr.append(mean_feature_acr)\n",
        "        features_arc = torch.cat(list_mean_feature_acr, dim=0)\n",
        "\n",
        "        # Concate featrues\n",
        "        features = torch.cat([features_cls, features_arc], dim=2)\n",
        "\n",
        "        logits = self.classifier(features)\n",
        "        outputs = ((logits),) + outputs[2:]\n",
        "\n",
        "        loss_fn = nn.BCELoss()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        if labels is not None:\n",
        "            total_loss = loss_fn(logits, labels)\n",
        "        \n",
        "        outputs = (total_loss,) + outputs\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "RrThzNb6SlQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, id, text, text_tokens, expansion, start_char_idx, length_acronym, start_token_idx, end_token_idx, label) -> None:\n",
        "        super().__init__()\n",
        "        self.guid = guid\n",
        "        self.id = id\n",
        "        self.text = text\n",
        "        self.text_tokens = text_tokens\n",
        "        self.expansion = expansion\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.length_acronym = length_acronym\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, id, input_ids, attention_mask, token_type_ids, start_token_idx, end_token_idx, label, expansion):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.label = label\n",
        "        self.expansion = expansion\n",
        "\n",
        "        self.id = id\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "class Processor(object):\n",
        "    \"\"\"Processor for the ArcBERT data set \"\"\"\n",
        "    def __init__(self, args) -> None:\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Reads json file\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    \n",
        "    def is_whitespace(self, c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def clean_syn(self, text):\n",
        "            text = re.sub('[\\?,\\.\\!:;\\(\\)]', '', text)\n",
        "            return text\n",
        "\n",
        "    def _create_examples(self, data, mode):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for i, example in enumerate(data):\n",
        "            guid = \"%s-%s\" % (mode, i)\n",
        "            id = example['id']\n",
        "            # 1. Input text\n",
        "            text = example['text']\n",
        "            text_tokens = []\n",
        "            char_to_word_offset = []\n",
        "            prev_is_whitespace = True\n",
        "            for c in text:\n",
        "                if self.is_whitespace(c):\n",
        "                    prev_is_whitespace = True\n",
        "                else:\n",
        "                    if prev_is_whitespace:\n",
        "                        text_tokens.append(c)\n",
        "                    else:\n",
        "                        text_tokens[-1] += c\n",
        "                    prev_is_whitespace = False\n",
        "                char_to_word_offset.append(len(text_tokens)-1)\n",
        "            # 2. Expansion of acr\n",
        "            expansion = example['expansion']\n",
        "            # 3. Position of acr and acr\n",
        "            start_char_idx = example['start_char_idx']\n",
        "            length_acronym = example['length_acronym']\n",
        "            start_token_idx = char_to_word_offset[start_char_idx]\n",
        "            end_token_idx = char_to_word_offset[start_char_idx+length_acronym-1]\n",
        "            # 4. Label\n",
        "            label = example['label']\n",
        "            examples.append(InputExample(\n",
        "                guid=guid,\n",
        "                id=id,\n",
        "                text=text,\n",
        "                text_tokens=text_tokens,\n",
        "                expansion=expansion,\n",
        "                start_char_idx=start_char_idx,\n",
        "                length_acronym=length_acronym,\n",
        "                start_token_idx=start_token_idx,\n",
        "                end_token_idx=end_token_idx,\n",
        "                label=label\n",
        "            ))\n",
        "        return examples\n",
        "    \n",
        "    def get_examples(self, args, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "\n",
        "        data_path = os.path.join(args.data_dir, mode)\n",
        "        data = self._read_file(os.path.join(data_path, args.data_file_name))\n",
        "        \n",
        "        PATH_DICTIONARY = os.path.join(args.data_dir, args.dict_file_name)\n",
        "        if not os.path.isfile(PATH_DICTIONARY):\n",
        "            raise Exception(f\"Folder {args.data_dir} doesn't contain canonical dictionary\")\n",
        "        dictionary = self._read_file(PATH_DICTIONARY)\n",
        "        \n",
        "        examples = []\n",
        "\n",
        "        pos_data = add_label_positive_sample(data)\n",
        "        examples.extend(pos_data)\n",
        "\n",
        "        neg_data = negative_data(pos_data, dictionary, mode)\n",
        "        examples.extend(neg_data)\n",
        "            \n",
        "        return self._create_examples(examples, mode)\n",
        "\n",
        "\n",
        "\n",
        "def negative_data(positive_data:list, diction:dict, mode) -> list:\n",
        "    \"\"\"\n",
        "    Funciton: Create negative samples\n",
        "    args:\n",
        "        positive_data: training data whose format {\n",
        "            'acronym': ...,(optional)\n",
        "            'expansion': ...,\n",
        "            'text': ...,\n",
        "            'start_char_idx: ...,\n",
        "            'lenght_acronym': ...,\n",
        "            'label': 1 (positive sample)\n",
        "        }\n",
        "        and\n",
        "        diction: dictionary of acronym and able expansion respectively\n",
        "    \"\"\"\n",
        "\n",
        "    neg_data = []\n",
        "    tmp = 0\n",
        "    for sample in positive_data:\n",
        "        try:\n",
        "            acronym = sample[\"text\"][sample[\"start_char_idx\"]:sample[\"start_char_idx\"]+sample['length_acronym']]\n",
        "            list_neg_expansion = diction[acronym].copy()\n",
        "            list_neg_expansion.remove(sample[\"expansion\"])\n",
        "            if mode == 'train':\n",
        "                if len(list_neg_expansion) > 1: \n",
        "                    list_neg_expansion = random.sample(list_neg_expansion, random.randint(1,2))\n",
        "            elif mode == 'dev' or mode == 'test':\n",
        "                if len(list_neg_expansion) > 1:\n",
        "                    list_neg_expansion = list_neg_expansion\n",
        "            for i in list_neg_expansion:\n",
        "                neg_data.append(sample.copy())\n",
        "                neg_data[tmp][\"expansion\"] = i\n",
        "                neg_data[tmp][\"label\"] = 0 # pseudo negative samples\n",
        "                tmp += 1\n",
        "        except: \n",
        "            print(sample)\n",
        "            continue\n",
        "    \n",
        "    return neg_data\n",
        "\n",
        "def add_label_positive_sample(data: list):\n",
        "    for idx, sample in enumerate(data):\n",
        "        sample['text'] = sample['text'].lower()\n",
        "        sample['label'] = 1\n",
        "        sample['id'] = idx\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                max_seq_len,\n",
        "                                tokenizer):\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    features = []\n",
        "    \n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 5000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "        \n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "\n",
        "        for (i, token) in enumerate(example.text_tokens):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "\n",
        "            for sub_token in sub_tokens:\n",
        "                all_doc_tokens.append(sub_token)\n",
        "        \n",
        "        start_token_idx = orig_to_tok_index[example.start_token_idx]\n",
        "        if len(orig_to_tok_index) == (example.end_token_idx + 1):\n",
        "            end_token_idx = orig_to_tok_index[-1]\n",
        "        else:\n",
        "            end_token_idx = orig_to_tok_index[example.end_token_idx + 1] - 1\n",
        "        \n",
        "        input_ids = []\n",
        "        \n",
        "        input_ids += [cls_token]\n",
        "        input_ids += all_doc_tokens\n",
        "        input_ids += [sep_token]\n",
        "        \n",
        "        token_type_ids = [0]*len(input_ids)\n",
        "        \n",
        "        expansion = example.expansion\n",
        "        expansion_tokens = tokenizer.tokenize(expansion)\n",
        "        \n",
        "        input_ids += expansion_tokens\n",
        "        input_ids += [sep_token]\n",
        "        \n",
        "\n",
        "        token_type_ids += [1]*(len(expansion_tokens) + 1)\n",
        "\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        \n",
        "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
        "        \n",
        "        padding = max_seq_len - len(input_ids)\n",
        "        \n",
        "        if padding < 0:\n",
        "            print('Ignore sample has length > 256 tokens')\n",
        "            continue\n",
        "        \n",
        "        input_ids = input_ids + ([pad_token_id] * padding)\n",
        "        attention_mask = attention_mask + [0]*padding\n",
        "        token_type_ids = token_type_ids + [0]*padding\n",
        "        assert len(input_ids) == len(attention_mask) == len(token_type_ids), \"Error with input length {} vs attention mask length {}, token type length {}\".format(len(input_ids), len(attention_mask), len(token_type_ids))\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
        "            len(attention_mask), max_seq_len\n",
        "        )\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n",
        "            len(token_type_ids), max_seq_len\n",
        "        )\n",
        "        id = example.id\n",
        "        label=example.label\n",
        "        \n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in all_doc_tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"labels: %s\" % \" \".join([str(x) for x in [label]]))\n",
        "            logger.info(\"expansion: %s\" % \" \".join([str(x) for x in [expansion]]))\n",
        "\n",
        "        \n",
        "        features.append(\n",
        "                InputFeatures(\n",
        "                    id = id,\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    token_type_ids=token_type_ids,\n",
        "                    start_token_idx=start_token_idx,\n",
        "                    end_token_idx=end_token_idx,\n",
        "                    label=label,\n",
        "                    expansion=expansion\n",
        "                )\n",
        "            )\n",
        "    return features\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode=None):\n",
        "    if not mode:\n",
        "        return None\n",
        "    processor = Processor(args)\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        'cached_{}_{}_{}'.format(mode,\n",
        "                                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                                args.max_seq_len\n",
        "                                )\n",
        "    )\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        \n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(args, \"train\")\n",
        "        elif mode == \"dev\":\n",
        "            examples = processor.get_examples(args, \"dev\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(args, \"test\")\n",
        "        else:\n",
        "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
        "            \n",
        "        features = convert_examples_to_features(\n",
        "            examples, args.max_seq_len, tokenizer\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.int64)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.float)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.int64)\n",
        "    all_start_token_idx = torch.tensor([f.start_token_idx for f in features], dtype=torch.int64)\n",
        "    all_end_token_idx = torch.tensor([f.end_token_idx for f in features], dtype=torch.int64)\n",
        "    all_label = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    all_id = torch.tensor([f.id for f in features], dtype=torch.long)\n",
        "    all_expansion = [f.expansion for f in features]\n",
        "\n",
        "    return (\n",
        "        all_input_ids,\n",
        "        all_token_type_ids,\n",
        "        all_attention_mask,\n",
        "        all_start_token_idx,\n",
        "        all_end_token_idx,\n",
        "        all_label,\n",
        "        all_id,\n",
        "        all_expansion\n",
        "    )\n",
        "\n",
        "\n",
        "class AcrDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                args,\n",
        "                tokenizer,\n",
        "                mode) -> None:\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        \n",
        "        self.dataset = load_and_cache_examples(args, tokenizer, mode)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset[0])\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        return  self.dataset[0][index], self.dataset[1][index], self.dataset[2][index], self.dataset[3][index], self.dataset[4][index], self.dataset[5][index], self.dataset[6][index], self.dataset[7][index]\n"
      ],
      "metadata": {
        "id": "_mO-OpBKSoXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_score_min = np.Inf\n",
        "\n",
        "    def __call__(self, score, model, args):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model, args)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model, args)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, score, model, args):\n",
        "        \"\"\"Saves model when validation loss decreases or accuracy/f1 increases.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'{args.tuning_metric} imporoved ({self.val_score_min:.6f} ----> {score:.6f}). Saving model ....')\n",
        "        model.save_pretrained(args.model_dir)\n",
        "        torch.save(args, os.path.join(args.model_dir, \"training_args.bin\"))\n",
        "        self.val_score_min = score"
      ],
      "metadata": {
        "id": "98MALvjcS00S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None) -> None:\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.train_dataset = train_dataset\n",
        "        self.dev_dataset = dev_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "        self.gold = read_json('/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/acrDrAid/data/gold.json')\n",
        "\n",
        "\n",
        "        self.config_class, self.model_class, self.tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "        if args.pretrained:\n",
        "            self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.token_level)\n",
        "            self.model = self.model_class.from_pretrained(\n",
        "                args.pretrained_path,\n",
        "                config=self.config,\n",
        "                args=self.args\n",
        "            )\n",
        "        else:\n",
        "            self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.token_level)\n",
        "            self.model = self.model_class.from_pretrained(\n",
        "                args.model_name_or_path,\n",
        "                config=self.config,\n",
        "                args=self.args\n",
        "            )\n",
        "\n",
        "        # GPU or CPU\n",
        "        torch.cuda.set_device(self.args.gpu_id)\n",
        "        print('GPU ID :',self.args.gpu_id)\n",
        "        print('Cuda device:',torch.cuda.current_device())\n",
        "        self.device = args.device\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        train_sampler = RandomSampler(self.train_dataset)\n",
        "        train_loader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
        "\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            self.args.num_train_epochs = (\n",
        "                self.args.max_steps // (len(train_loader) // self.args.gradient_accumulation_steps) + 1\n",
        "            )\n",
        "        else:\n",
        "            t_total = len(train_loader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
        "        \n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': self.args.weight_decay},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.Adam(lr=self.args.learning_rate, betas=(0.9, 0.98), eps=self.args.adam_epsilon, params=optimizer_grouped_parameters)\n",
        "        \n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "\n",
        "\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
        "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
        "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
        "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
        "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
        "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
        "\n",
        "        global_step = 0\n",
        "        tr_loss = 0.0\n",
        "        # self.model.zero_grad()\n",
        "\n",
        "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
        "        early_stopping = EarlyStopping(patience=self.args.early_stopping, verbose=True)\n",
        "\n",
        "        for it in train_iterator:\n",
        "            print(f'epoch: {it}')\n",
        "            epoch_iterator = tqdm(train_loader, desc=\"Iteration\", position=0, leave=True)\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                self.model.train()\n",
        "                # batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0].to(self.device),\n",
        "                    \"token_type_ids\": batch[1].to(self.device),\n",
        "                    \"attention_mask\": batch[2].to(self.device),\n",
        "                    \"start_token_idx\": batch[3].to(self.device),\n",
        "                    \"end_token_idx\": batch[4].to(self.device),\n",
        "                    \"labels\": batch[5].to(self.device)\n",
        "                }\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(**inputs)\n",
        "                loss, _ = outputs[:2]\n",
        "\n",
        "                if self.args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "                loss.backward()\n",
        "                tr_loss += loss.item()\n",
        "                \n",
        "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
        "                        print(\"\\nTuning metrics:\", self.args.tuning_metric)\n",
        "                        eval_loss, ids, pred_expansions, pred_scores = self.evaluate('test')\n",
        "                        results = compute_metrics(self.args, ids, pred_expansions, pred_scores)\n",
        "                        results['loss'] = eval_loss\n",
        "                        logger.info(\"***** Eval results *****\")\n",
        "                        for key in sorted(results.keys()):\n",
        "                            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "                        early_stopping(results[self.args.tuning_metric], self.model, self.args)\n",
        "                        if early_stopping.early_stop:\n",
        "                            print('Early Stopping')\n",
        "                            break\n",
        "                            \n",
        "                        print(f'Training Loss {tr_loss / global_step}')\n",
        "                    \n",
        "                    # if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
        "                    #     self.save_model()\n",
        "                \n",
        "                if 0 < self.args.max_steps < global_step:\n",
        "                    epoch_iterator.close()\n",
        "                    break\n",
        "            if 0 < self.args.max_steps < global_step or early_stopping.early_stop:\n",
        "                train_iterator.close()\n",
        "                break\n",
        "        \n",
        "        return global_step, tr_loss / global_step\n",
        "    \n",
        "    def write_evaluation_result(self, out_file, results):\n",
        "        out_file = self.args.model_dir + \"/\" + out_file\n",
        "        w = open(out_file, \"w\", encoding=\"utf-8\")\n",
        "        w.write(\"***** Eval results *****\\n\")\n",
        "        for key in sorted(results.keys()):\n",
        "            to_write = \" {key} = {value}\".format(key=key, value=str(results[key]))\n",
        "            w.write(to_write)\n",
        "            w.write(\"\\n\")\n",
        "        w.close()\n",
        "\n",
        "    def evaluate(self, mode):\n",
        "        if mode == 'test':\n",
        "            dataset = self.test_dataset\n",
        "        elif mode == 'dev':\n",
        "            dataset = self.dev_dataset\n",
        "        else:\n",
        "            raise Exception(\"Only dev and test dataset available\")\n",
        "        \n",
        "        eval_sampler = SequentialSampler(dataset)\n",
        "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
        "\n",
        "        # Eval\n",
        "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
        "        logger.info(\"  Num examples = %d\", len(dataset))\n",
        "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
        "\n",
        "        ids = []\n",
        "        pred_expansions = []\n",
        "        pred_scores = []\n",
        "\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        correct = 0\n",
        "\n",
        "        self.model.eval()\n",
        "        \n",
        "\n",
        "        for batch in eval_dataloader:\n",
        "            # batch = tuple(t.to(self.device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0].to(self.device),\n",
        "                    \"token_type_ids\": batch[1].to(self.device),\n",
        "                    \"attention_mask\": batch[2].to(self.device),\n",
        "                    \"start_token_idx\": batch[3].to(self.device),\n",
        "                    \"end_token_idx\": batch[4].to(self.device),\n",
        "                    \"labels\": batch[5].to(self.device)\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                loss, logits = outputs[:2]\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                labels = inputs['labels']\n",
        "                preds = (logits > self.args.threshold).type(torch.int16)\n",
        "                correct += sum(preds == labels).item()\n",
        "\n",
        "                ids.extend(batch[6].tolist())\n",
        "                pred_expansions.extend(list(batch[7]))\n",
        "                \n",
        "                pred_scores.extend(logits.tolist())\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "        \n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        acc = correct/len(dataset)\n",
        "        print(f'Classification Accuracy: {acc}')\n",
        "\n",
        "        return eval_loss, ids, pred_expansions, pred_scores\n",
        "    \n",
        "    def load_model(self):\n",
        "        # Check whether model exists\n",
        "        if not os.path.exists(self.args.model_dir):\n",
        "            raise Exception(\"Model doesn't exists! Train first!\")\n",
        "\n",
        "        try:\n",
        "            self.model = self.model_class.from_pretrained(\n",
        "                self.args.model_dir,\n",
        "                config=self.config,\n",
        "                args=self.args\n",
        "                \n",
        "            )\n",
        "            self.model.to(self.device)\n",
        "            logger.info(\"***** Model Loaded *****\")\n",
        "        except Exception:\n",
        "            raise Exception(\"Some model files might be missing...\")"
      ],
      "metadata": {
        "id": "aXF3J3jIS53n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    RobertaConfig\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"vihnbert\": (RobertaConfig, ViHnBERT, AutoTokenizer),\n",
        "    \"phobert\": (RobertaConfig, ViHnBERT, AutoTokenizer)\n",
        "    }\n",
        "\n",
        "MODEL_PATH_MAP = {\n",
        "    \"vihnbert\": \"demdecuong/vihealthbert-base-word\",\n",
        "    \"phobert\": \"vinai/phobert-base\"\n",
        "    }\n",
        "\n",
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def load_tokenizer(args):\n",
        "    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n",
        "\n",
        "def score_expansion(gold, prediction):\n",
        "    \"\"\"\n",
        "    gold, prediction is list\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for i in range(len(gold)):\n",
        "        if gold[i] == prediction[i]:\n",
        "            correct += 1\n",
        "    acc = correct/len(prediction)\n",
        "\n",
        "    expansions = set()\n",
        "    correct_per_expansion = defaultdict(int)\n",
        "    total_per_expansion = defaultdict(int)\n",
        "    pred_per_expansion = defaultdict(int)\n",
        "\n",
        "    for i in range(len(gold)):\n",
        "        expansions.add(gold[i])\n",
        "        total_per_expansion[gold[i]] += 1\n",
        "        pred_per_expansion[prediction[i]] += 1\n",
        "        if gold[i] == prediction[i]:\n",
        "            correct_per_expansion[gold[i]] += 1\n",
        "    \n",
        "    precs = defaultdict(int)\n",
        "    recalls = defaultdict(int)\n",
        "\n",
        "    for exp in expansions:\n",
        "        precs[exp] = correct_per_expansion[exp] / pred_per_expansion[exp] if exp in pred_per_expansion else 1\n",
        "        recalls[exp] = correct_per_expansion[exp] / total_per_expansion[exp]\n",
        "\n",
        "    # micro-pred = micro-recall = micro-f1 = acc if len(gold) = len(prediction)\n",
        "    micro_prec = sum(correct_per_expansion.values()) / sum(pred_per_expansion.values())\n",
        "    micro_recall = sum(correct_per_expansion.values()) / sum(total_per_expansion.values())\n",
        "    micro_f1 = 2*micro_prec*micro_recall/(micro_prec+micro_recall) if micro_prec+micro_recall != 0 else 0\n",
        "\n",
        "    # official evaluation metrics are the macro-averaged precision, recall and F1 for correct expansion predictions\n",
        "    macro_prec = sum(precs.values()) / len(precs)\n",
        "    macro_recall = sum(recalls.values()) / len(recalls)\n",
        "    macro_f1 = 2*macro_prec*macro_recall / (macro_prec+macro_recall) if macro_prec+macro_recall != 0 else 0\n",
        "\n",
        "    return macro_prec, macro_recall, macro_f1, acc\n",
        "\n",
        "def compute_metrics(args, ids, pred_expansions, pred_scores):\n",
        "    thresh = args.threshold\n",
        "    gold = read_json(os.path.join(args.data_dir, args.gold_file_name))\n",
        "    pred = {}\n",
        "    for i, expan, score in zip(ids, pred_expansions, pred_scores):\n",
        "        if score > thresh:\n",
        "            if i not in pred:\n",
        "                pred[i] = [score, expan]\n",
        "            else:\n",
        "                if score > pred[i][0]:\n",
        "                    pred[i] = [score, expan]\n",
        "    pred = [pred[int(k)][1] if int(k) in pred else '' for k,v in gold.items()]\n",
        "    gold = [gold[k] for k,v in gold.items()]\n",
        "    assert len(gold) == len(pred)\n",
        "    macro_prec, macro_recall, macro_f1, acc = score_expansion(gold, pred)\n",
        "    result = {}\n",
        "\n",
        "    result['macro_prec'] = macro_prec\n",
        "    result['macro_recall'] = macro_recall\n",
        "    result['macro_f1'] = macro_f1\n",
        "    result['accuracy'] = acc\n",
        "\n",
        "    return result\n",
        "    \n",
        "def read_json(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "6TMNgOEXTAeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "    tokenizer = load_tokenizer(args)\n",
        "\n",
        "    train_dataset = AcrDataset(args, tokenizer, 'train')\n",
        "    dev_dataset = AcrDataset(args, tokenizer, 'dev')\n",
        "    test_dataset = AcrDataset(args, tokenizer, 'test')\n",
        "\n",
        "    trainer = Trainer(args, train_dataset, dev_dataset, test_dataset)\n",
        "\n",
        "    eval_loss, ids, pred_expansions, pred_scores = trainer.evaluate('test')\n",
        "    results = compute_metrics(args, ids, pred_expansions, pred_scores)\n",
        "    results['loss'] = eval_loss\n",
        "    logger.info(\"***** Eval results *****\")\n",
        "    for key in sorted(results.keys()):\n",
        "        logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "    print(f\"Anh Viet Pham do_train: {args.do_train}\")\n",
        "\n",
        "    print(f\"Anh Viet Pham do_eval: {args.do_eval}\")\n",
        "\n",
        "\n",
        "    if args.do_train:\n",
        "        print(\"Anh Viet Pham do_train\")\n",
        "        trainer.train()\n",
        "\n",
        "    if args.do_eval:\n",
        "        trainer.load_model()\n",
        "        eval_loss, ids, pred_expansions, pred_scores = trainer.evaluate('dev')\n",
        "        results = compute_metrics(args, ids, pred_expansions, pred_scores)\n",
        "        results['loss'] = eval_loss\n",
        "        logger.info(\"***** Dev results *****\")\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "        eval_loss, ids, pred_expansions, pred_scores = trainer.evaluate('test')\n",
        "        results = compute_metrics(args, ids, pred_expansions, pred_scores)\n",
        "        results['loss'] = eval_loss\n",
        "        logger.info(\"***** Test results *****\")\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(results[key]))"
      ],
      "metadata": {
        "id": "RqglSVGcTL0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# parser.add_argument(\"--task\", default=None, required=True, type=str, help=\"The name of the task to train\")\n",
        "parser.add_argument(\"--model_dir\", default=\"/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/model-save\", type=str, help=\"Path to save, load model\")\n",
        "parser.add_argument(\"--data_dir\", default=\"/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/acrDrAid/data\", type=str, help=\"The input data dir\")\n",
        "parser.add_argument(\"--data_file_name\", default=\"data.json\", type=str, help=\"The input data name\")\n",
        "parser.add_argument(\"--gold_file_name\", default=\"gold.json\", type=str, help=\"The gold file name\")\n",
        "parser.add_argument(\"--dict_file_name\", default=\"dictionary.json\", type=str, help=\"The dictionary file name\")\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--model_type\",\n",
        "        default=\"phobert\",\n",
        "        type=str,\n",
        "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        ")\n",
        "parser.add_argument(\"--tuning_metric\", default=\"macro_f1\", type=str, help=\"Metrics to tune when training\")\n",
        "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "parser.add_argument(\"--train_batch_size\", default=32, type=int, help=\"Batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int, help=\"Batch size for evaluation.\")\n",
        "parser.add_argument(\n",
        "        \"--max_seq_len\", default=256, type=int, help=\"The maximum total input sequence length after tokenization.\"\n",
        ")\n",
        "parser.add_argument(\"--learning_rate\", default=1e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "parser.add_argument(\n",
        "        \"--num_train_epochs\", default=100.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        ")\n",
        "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        ")\n",
        "parser.add_argument(\"--adam_epsilon\", default=1e-9, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        ")\n",
        "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
        "\n",
        "parser.add_argument(\"--logging_steps\", type=int, default=200, help=\"Log every X updates steps.\")\n",
        "parser.add_argument(\"--save_steps\", type=int, default=200, help=\"Save checkpoint every X updates steps.\")\n",
        "\n",
        "parser.add_argument(\"--do_train\", action=\"store_true\", default=False, help=\"Whether to run training.\")\n",
        "parser.add_argument(\"--do_eval\", action=\"store_true\", default=True, help=\"Whether to run eval on the test set.\")\n",
        "\n",
        "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--token_level\",\n",
        "        type=str,\n",
        "        default=\"word-level\",\n",
        "        help=\"Tokens are at syllable level or word level (Vietnamese) [word-level, syllable-level]\",\n",
        ")\n",
        "parser.add_argument(\n",
        "        \"--early_stopping\",\n",
        "        type=int,\n",
        "        default=25,\n",
        "        help=\"Number of unincreased validation step to wait for early stopping\",\n",
        ")\n",
        "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"Select gpu id\")\n",
        "        \n",
        "# init pretrained\n",
        "parser.add_argument(\"--pretrained\", action=\"store_true\", help=\"Whether to init model from pretrained base model\")\n",
        "parser.add_argument(\"--pretrained_path\", default=\"./workspace/vinbrain/vutran/Transfer_Learning/Domain_Adaptive/Finetune/WSD/src/XLMr_ADvn/1e-5/42/\", type=str, help=\"The pretrained model path\")\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--threshold\", default=0.5, type=float, help=\"Threshold\"\n",
        ")\n",
        "\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM6BegPNTUav",
        "outputId": "bbf9856d-10b4-4416-874d-59d59575b8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing ViHnBERT: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing ViHnBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViHnBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViHnBERT were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense_1.bias', 'classifier.dense_1.weight', 'classifier.dense_2.weight', 'classifier.dense_2.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ID : 0\n",
            "Cuda device: 0\n",
            "Classification Accuracy: 0.4544130498106612\n",
            "Anh Viet Pham do_train: False\n",
            "Anh Viet Pham do_eval: True\n",
            "Classification Accuracy: 0.921968787515006\n",
            "Classification Accuracy: 0.8598893096417128\n"
          ]
        }
      ]
    }
  ]
}