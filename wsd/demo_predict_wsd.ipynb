{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8KF8WgUoD7DuhbqjJYCOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnhVietPham/Text-Mining/blob/main/wsd/demo_predict_wsd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbrETYpKRLvi",
        "outputId": "cfc92933-d869-4592-94a5-a76214f56a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "id": "nXnUupm0RObF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4SpGnUwSSWW",
        "outputId": "c4f76010-672c-400e-fb82-2b8d45a05102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFW6gLrlQ6fG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaPreTrainedModel, RobertaModel, AutoTokenizer, RobertaConfig\n",
        "from underthesea import word_tokenize\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, config, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout_rate * 2)\n",
        "        self.dense_1 = nn.Linear(config.hidden_size * 2, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_2 = nn.Dropout(dropout_rate)\n",
        "        self.dense_2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, feature):\n",
        "        feature = self.dropout_1(feature)\n",
        "        feature = self.dense_1(feature)\n",
        "        feature = self.relu(feature)\n",
        "        feature = self.dropout_2(feature)\n",
        "        feature = self.dense_2(feature).view(-1)\n",
        "\n",
        "        feature = self.sigmoid(feature)\n",
        "        return feature"
      ],
      "metadata": {
        "id": "kDFI70uYRDTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViHnBERT(RobertaPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(ViHnBERT, self).__init__(config)\n",
        "        self.args = args\n",
        "        self.config = config\n",
        "        # init backbone\n",
        "        self.roberta = RobertaModel(config)\n",
        "\n",
        "        self.classifier = Classifier(config, args.dropout_rate)\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids=None,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=None,\n",
        "                start_token_idx=None,\n",
        "                end_token_idx=None,\n",
        "                labels=None):\n",
        "\n",
        "        outputs = self.roberta(input_ids=input_ids,\n",
        "                               attention_mask=attention_mask)\n",
        "\n",
        "        features_bert = outputs[0]\n",
        "        # Features of [CLS] tokens\n",
        "        features_cls = features_bert[:, 0, :].unsqueeze(1)\n",
        "\n",
        "        # Features of acronym tokens\n",
        "        if start_token_idx is None or end_token_idx is None:\n",
        "            raise Exception('Require start_token_idx and end_token_idx')\n",
        "        list_mean_feature_acr = []\n",
        "        for idx in range(features_bert.size()[0]):\n",
        "            feature_acr = features_bert[idx, start_token_idx[idx]:end_token_idx[idx] + 1, :].unsqueeze(0)\n",
        "            mean_feature_acr = torch.mean(feature_acr, 1, True)\n",
        "            list_mean_feature_acr.append(mean_feature_acr)\n",
        "        features_arc = torch.cat(list_mean_feature_acr, dim=0)\n",
        "\n",
        "        # Concate featrues\n",
        "        features = torch.cat([features_cls, features_arc], dim=2)\n",
        "\n",
        "        logits = self.classifier(features)\n",
        "        outputs = ((logits),) + outputs[2:]\n",
        "\n",
        "        loss_fn = nn.BCELoss()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        if labels is not None:\n",
        "            total_loss = loss_fn(logits, labels)\n",
        "\n",
        "        outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "7vHNgBTFRbkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "    def __init__(self, guid, id, text, text_tokens, expansion, start_char_idx, length_acronym, start_token_idx,\n",
        "                 end_token_idx, label) -> None:\n",
        "        super().__init__()\n",
        "        self.guid = guid\n",
        "        self.id = id\n",
        "        self.text = text\n",
        "        self.text_tokens = text_tokens\n",
        "        self.expansion = expansion\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.length_acronym = length_acronym\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "metadata": {
        "id": "oFF8eSxCRd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, id, input_ids, attention_mask, token_type_ids, start_token_idx, end_token_idx, label, expansion):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.label = label\n",
        "        self.expansion = expansion\n",
        "\n",
        "        self.id = id\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "metadata": {
        "id": "usX2CmE-Rgve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _read_file(input_file):\n",
        "    \"\"\"Reads json file\"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "k8yB-TdlRjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_label_positive_sample(data: list):\n",
        "    for idx, sample in enumerate(data):\n",
        "        sample['text'] = sample['text'].lower()\n",
        "        sample['label'] = 1\n",
        "        sample['id'] = idx\n",
        "    return data\n",
        "\n",
        "\n",
        "def negative_data(positive_data: list, diction: dict) -> list:\n",
        "    neg_data = []\n",
        "    tmp = 0\n",
        "    for sample in positive_data:\n",
        "        try:\n",
        "            acronym = sample[\"text\"][sample[\"start_char_idx\"]:sample[\"start_char_idx\"] + sample['length_acronym']]\n",
        "            list_neg_expansion = diction[acronym].copy()\n",
        "            if len(list_neg_expansion) > 1:\n",
        "                list_neg_expansion = list_neg_expansion\n",
        "            for i in list_neg_expansion:\n",
        "                neg_data.append(sample.copy())\n",
        "                neg_data[tmp][\"expansion\"] = i\n",
        "                neg_data[tmp][\"label\"] = 0  # pseudo negative samples\n",
        "                tmp += 1\n",
        "        except:\n",
        "            print(sample)\n",
        "            continue\n",
        "\n",
        "    return neg_data"
      ],
      "metadata": {
        "id": "09RbOmrARlxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_examples(data):\n",
        "    examples = []\n",
        "    for i, example in enumerate(data):\n",
        "        guid = \"\"\n",
        "        id = example['id']\n",
        "        # 1. Input text\n",
        "        text = example['text']\n",
        "        text_tokens = []\n",
        "        char_to_word_offset = []\n",
        "        prev_is_whitespace = True\n",
        "        for c in text:\n",
        "            if is_whitespace(c):\n",
        "                prev_is_whitespace = True\n",
        "            else:\n",
        "                if prev_is_whitespace:\n",
        "                    text_tokens.append(c)\n",
        "                else:\n",
        "                    text_tokens[-1] += c\n",
        "                prev_is_whitespace = False\n",
        "            char_to_word_offset.append(len(text_tokens) - 1)\n",
        "        # 2. Expansion of acr\n",
        "        expansion = example['expansion']\n",
        "        # 3. Position of acr and acr\n",
        "        start_char_idx = example['start_char_idx']\n",
        "        length_acronym = example['length_acronym']\n",
        "        start_token_idx = char_to_word_offset[start_char_idx]\n",
        "        end_token_idx = char_to_word_offset[start_char_idx + length_acronym - 1]\n",
        "        # 4. Label\n",
        "        label = example['label']\n",
        "        examples.append(InputExample(\n",
        "            guid=guid,\n",
        "            id=id,\n",
        "            text=text,\n",
        "            text_tokens=text_tokens,\n",
        "            expansion=expansion,\n",
        "            start_char_idx=start_char_idx,\n",
        "            length_acronym=length_acronym,\n",
        "            start_token_idx=start_token_idx,\n",
        "            end_token_idx=end_token_idx,\n",
        "            label=label\n",
        "        ))\n",
        "    return examples"
      ],
      "metadata": {
        "id": "ZDFePxgURt7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_example(data):\n",
        "    dictionary = _read_file(\n",
        "        \"/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/wsd/dictionary.json\")\n",
        "\n",
        "    examples = []\n",
        "\n",
        "    pos_data = add_label_positive_sample(data)\n",
        "    neg_data = negative_data(pos_data, dictionary)\n",
        "    examples.extend(neg_data)\n",
        "    return _create_examples(examples)"
      ],
      "metadata": {
        "id": "iN5WT4VgRppk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples,\n",
        "                                 max_seq_len,\n",
        "                                 tokenizer):\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "\n",
        "        for (i, token) in enumerate(example.text_tokens):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "\n",
        "            for sub_token in sub_tokens:\n",
        "                all_doc_tokens.append(sub_token)\n",
        "\n",
        "        start_token_idx = orig_to_tok_index[example.start_token_idx]\n",
        "        if len(orig_to_tok_index) == (example.end_token_idx + 1):\n",
        "            end_token_idx = orig_to_tok_index[-1]\n",
        "        else:\n",
        "            end_token_idx = orig_to_tok_index[example.end_token_idx + 1] - 1\n",
        "\n",
        "        input_ids = []\n",
        "\n",
        "        input_ids += [cls_token]\n",
        "        input_ids += all_doc_tokens\n",
        "        input_ids += [sep_token]\n",
        "\n",
        "        token_type_ids = [0] * len(input_ids)\n",
        "\n",
        "        expansion = example.expansion\n",
        "        expansion_tokens = tokenizer.tokenize(expansion)\n",
        "\n",
        "        input_ids += expansion_tokens\n",
        "        input_ids += [sep_token]\n",
        "\n",
        "        token_type_ids += [1] * (len(expansion_tokens) + 1)\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
        "\n",
        "        padding = max_seq_len - len(input_ids)\n",
        "\n",
        "        if padding < 0:\n",
        "            print('Ignore sample has length > 256 tokens')\n",
        "            continue\n",
        "\n",
        "        input_ids = input_ids + ([pad_token_id] * padding)\n",
        "        attention_mask = attention_mask + [0] * padding\n",
        "        token_type_ids = token_type_ids + [0] * padding\n",
        "        assert len(input_ids) == len(attention_mask) == len(\n",
        "            token_type_ids), \"Error with input length {} vs attention mask length {}, token type length {}\".format(\n",
        "            len(input_ids), len(attention_mask), len(token_type_ids))\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
        "            len(attention_mask), max_seq_len\n",
        "        )\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n",
        "            len(token_type_ids), max_seq_len\n",
        "        )\n",
        "        id = example.id\n",
        "        label = example.label\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                id=id,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                start_token_idx=start_token_idx,\n",
        "                end_token_idx=end_token_idx,\n",
        "                label=label,\n",
        "                expansion=expansion\n",
        "            )\n",
        "        )\n",
        "    return features"
      ],
      "metadata": {
        "id": "ZJn2sE-rRvy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"phình thoát vị nhỏ, đa tầng các đđ c4/c5 (nh), c5/c6 (thể cạnh trung tâm lệch trái), c6/c7 (nh) ra sau, đè ép nhẹ mặt trước bao màng cứng, gây hẹp ống sống, chèn ép nhẹ tủy cổ ngang mức nhưng chưa gây phù tủy (đường kính ống sống trước-sau)\"\n",
        "tokenize = word_tokenize(text)\n",
        "print(tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeObY6z5UGHd",
        "outputId": "8e62c99c-6d10-4259-ca12-889ea2543ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['phình', 'thoát', 'vị', 'nhỏ', ',', 'đa', 'tầng', 'các', 'đđ', 'c4', '/', 'c5', '(', 'nh', ')', ',', 'c5', '/', 'c6', '(', 'thể', 'cạnh', 'trung tâm', 'lệch', 'trái', ')', ',', 'c6', '/', 'c7', '(', 'nh', ')', 'ra', 'sau', ',', 'đè ép', 'nhẹ', 'mặt', 'trước', 'bao', 'màng cứng', ',', 'gây', 'hẹp', 'ống', 'sống', ',', 'chèn ép', 'nhẹ', 'tủy', 'cổ', 'ngang', 'mức', 'nhưng', 'chưa', 'gây', 'phù', 'tủy', '(', 'đường kính', 'ống', 'sống', 'trước-sau', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \" \".join(tokenize)\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqbBzw2WVdvv",
        "outputId": "ecf70d11-5411-4b97-840f-5b96efbede6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phình thoát vị nhỏ , đa tầng các đđ c4 / c5 ( nh ) , c5 / c6 ( thể cạnh trung tâm lệch trái ) , c6 / c7 ( nh ) ra sau , đè ép nhẹ mặt trước bao màng cứng , gây hẹp ống sống , chèn ép nhẹ tủy cổ ngang mức nhưng chưa gây phù tủy ( đường kính ống sống trước-sau )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/wsd/dictionary.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "a = \"\"\n",
        "for k, v in data.items():\n",
        "    if k in tokenize:\n",
        "        a = k\n",
        "json_data_list = []\n",
        "json_data = {\n",
        "        \"id\": 0,\n",
        "        \"text\": input_text,\n",
        "        \"start_char_idx\": input_text.index(a),\n",
        "        \"length_acronym\": len(a),\n",
        "        \"expansion\": \"\",\n",
        "        \"label\": 1\n",
        "}\n",
        "\n",
        "json_data_list.append(json_data)\n",
        "pretrain_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "examples = get_example(json_data_list)\n",
        "features = convert_examples_to_features(examples, 256, pretrain_tokenizer)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.int64)\n",
        "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.float)\n",
        "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.int64)\n",
        "all_start_token_idx = torch.tensor([f.start_token_idx for f in features], dtype=torch.int64)\n",
        "all_end_token_idx = torch.tensor([f.end_token_idx for f in features], dtype=torch.int64)\n",
        "all_label = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "all_id = torch.tensor([f.id for f in features], dtype=torch.long)\n",
        "all_expansion = [f.expansion for f in features]\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
        "parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
        "parser.add_argument('-f')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "config = RobertaConfig.from_pretrained('vinai/phobert-base', finetuning_task='')\n",
        "model = ViHnBERT.from_pretrained('/content/drive/MyDrive/Luận Văn Thạc Sĩ/data-vihealbert/wsd/model-save-wsd1',\n",
        "    config=config,\n",
        "    args=args,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkQabz2ORzHP",
        "outputId": "cdcca9e0-e73d-47f1-b4b3-a39ec6479695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"token_type_ids\": all_token_type_ids,\n",
        "        \"attention_mask\": all_attention_mask,\n",
        "        \"start_token_idx\": all_start_token_idx,\n",
        "        \"end_token_idx\": all_end_token_idx,\n",
        "        \"labels\": all_label\n",
        "}\n",
        "\n",
        "\n",
        "outputs = model(**inputs)\n",
        "tmp_eval_loss, (slot_logits) = outputs[:2]\n",
        "slot_preds = slot_logits.detach().cpu().numpy()\n",
        "slot_preds = np.argmax(slot_preds)"
      ],
      "metadata": {
        "id": "iq27XvxCSp6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Sentence: {text}')\n",
        "print(f'acronym: {a}')\n",
        "print(f'start_char_idx: {text.index(a)}')\n",
        "print(f'length_acronym: {len(a)}')\n",
        "print(f'expansion: {data[a][slot_preds]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7G6l47IS1HC",
        "outputId": "a1659aab-6766-4309-ac9a-8fb7a02984a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: phình thoát vị nhỏ, đa tầng các đđ c4/c5 (nh), c5/c6 (thể cạnh trung tâm lệch trái), c6/c7 (nh) ra sau, đè ép nhẹ mặt trước bao màng cứng, gây hẹp ống sống, chèn ép nhẹ tủy cổ ngang mức nhưng chưa gây phù tủy (đường kính ống sống trước-sau)\n",
            "acronym: đđ\n",
            "start_char_idx: 32\n",
            "length_acronym: 2\n",
            "expansion: đĩa đệm\n"
          ]
        }
      ]
    }
  ]
}